{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ac4d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ stream error: 'int' object has no attribute 'get'\n",
      "⚠️ stream error: 'int' object has no attribute 'get'\n",
      "✅ Collected 52 edits in the last 15 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_120702/3636650258.py:144: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  time_edits = pd.concat([time_edits,new_row])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 141\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m#continuous loop\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     batch_df = \u001b[43mcollect_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBATCH_SECONDS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Collected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batch_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m edits in the last \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBATCH_SECONDS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds.\u001b[39m\u001b[33m\"\u001b[39m) \n\u001b[32m    143\u001b[39m     new_row = pd.DataFrame({\u001b[33m'\u001b[39m\u001b[33mNumber of edits\u001b[39m\u001b[33m'\u001b[39m: [\u001b[38;5;28mlen\u001b[39m(batch_df)], \u001b[33m'\u001b[39m\u001b[33mMinute\u001b[39m\u001b[33m'\u001b[39m: [minute], \u001b[33m'\u001b[39m\u001b[33mAnomaly\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0\u001b[39m]})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mcollect_for\u001b[39m\u001b[34m(seconds)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m EventSource(STREAM_URL, headers=headers) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/requests_sse/client.py:203\u001b[39m, in \u001b[36mEventSource.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         line_in_bytes = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    205\u001b[39m         \u001b[38;5;28mself\u001b[39m._event_type = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/requests/models.py:869\u001b[39m, in \u001b[36mResponse.iter_lines\u001b[39m\u001b[34m(self, chunk_size, decode_unicode, delimiter)\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[32m    861\u001b[39m \u001b[33;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[32m    862\u001b[39m \u001b[33;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[32m    863\u001b[39m \n\u001b[32m    864\u001b[39m \u001b[33;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[32m    865\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    867\u001b[39m pending = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_unicode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_unicode\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/urllib3/response.py:1088\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1072\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/urllib3/response.py:1248\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1245\u001b[39m     amt = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1250\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/urllib3/response.py:1167\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1168\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/ssl.py:1253\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1250\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1251\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1252\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1255\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1107\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "from requests_sse import EventSource\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "url= 'https://stream.wikimedia.org/v2/stream/mediawiki.recentchange'\n",
    "\n",
    "# Adding headers can help in case the server requires specific request formatting\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Function to determine if the change is to a talk page\n",
    "def is_talk_page(title):\n",
    "    # Typically, talk pages start with \"Talk:\" or \"<Language> talk:\"\n",
    "    # This will handle \"Talk:\", \"User talk:\", \"Wikipedia talk:\", etc.\n",
    "    return any(title.lower().startswith(prefix) for prefix in ['talk:', 'wikipedia talk:', 'file talk:', \n",
    "                                                              'template talk:', 'help talk:', 'category talk:', 'portal talk:',\n",
    "                                                              'book talk:', 'draft talk:', 'timedtext talk:', 'module talk:'])\n",
    "# Helper function to clean the talk prefix\n",
    "def clean_talk_prefix(title):\n",
    "    prefixes = ['talk:', 'wikipedia talk:', 'file talk:', 'template talk:',\n",
    "                'help talk:', 'category talk:', 'portal talk:', 'book talk:',\n",
    "                'draft talk:', 'timedtext talk:', 'module talk:']\n",
    "    title_lower = title.lower()\n",
    "    for prefix in prefixes:\n",
    "        if title_lower.startswith(prefix):\n",
    "            return title[len(prefix):].strip()  # Remove prefix and extra spaces\n",
    "    return title\n",
    "\n",
    "data_list= [[],[],[]]\n",
    "t_end = time.time() + 60\n",
    "# Setting up the EventSource connection\n",
    "\n",
    "with EventSource(url, headers=headers) as stream:\n",
    "    for event in stream:\n",
    "        if time.time() > t_end:\n",
    "            break\n",
    "\n",
    "        if event.type == 'message':\n",
    "            try:\n",
    "                # Parse the event data as JSON\n",
    "                change = json.loads(event.data)\n",
    "                # Check if the change is related to a talk page from Wikipedia\n",
    "            \n",
    "                if change['wiki'].endswith('wiki') and is_talk_page(change['title']) and change['bot'] == False and change['wiki']=='enwiki':\n",
    "                    #get the number of bytes\n",
    "                    old_len = change.get('length', {}).get('old') #old bytes\n",
    "                    new_len = change.get('length', {}).get('new') #edited bytes\n",
    "                    if old_len is not None and new_len is not None:\n",
    "                        byte_diff = abs(new_len - old_len)\n",
    "                    else:\n",
    "                        byte_diff = None #in case no data\n",
    "                \n",
    "                    print('{user} edited {title}: {comment} with {byte_diff} bytes'.format(\n",
    "                        user=change['user'], title=clean_talk_prefix(change['title']), comment = change['comment'], byte_diff = byte_diff))\n",
    "                    data_list[0].append(clean_talk_prefix(change['title']))\n",
    "                    data_list[1].append(change['comment'])\n",
    "                    data_list[2].append(byte_diff)\n",
    "                   \n",
    "                    data = {\n",
    "                        \"Title\": data_list[0],\n",
    "                        \"Comment\": data_list[1],\n",
    "                        \"Bytes\": data_list[2]\n",
    "                    }\n",
    "            \n",
    "            except ValueError: \n",
    "                # In case of any issues in parsing JSON data\n",
    "                continue\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('edit.csv',index = False)\n",
    "'''\n",
    "BATCH_SECONDS = 15\n",
    "STREAM_URL = 'https://stream.wikimedia.org/v2/stream/mediawiki.recentchange'\n",
    "def collect_for(seconds=BATCH_SECONDS):\n",
    "    \"\"\"Collect ANY recent changes for a fixed time window (nearly no filtering).\"\"\"\n",
    "    headers = {\"User-Agent\": \"HTB-Headlines/1.0 (demo)\", \"Accept\": \"text/event-stream\"}\n",
    "    edits = []\n",
    "    start = time.time()\n",
    "    while time.time() - start < seconds:  # ✅ keep trying until full 60s\n",
    "        try:\n",
    "            with EventSource(STREAM_URL, headers=headers) as stream:\n",
    "                for event in stream:\n",
    "                    if time.time() - start >= seconds:\n",
    "                        break\n",
    "                    if event.type != \"message\" or not event.data:\n",
    "                        continue\n",
    "                    try:\n",
    "                        change = json.loads(event.data)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "                    if not isinstance(change, dict):\n",
    "                        continue\n",
    "                    if change.get(\"wiki\") != \"enwiki\":\n",
    "                        continue\n",
    "\n",
    "                    title = str(change.get(\"title\", \"\") or \"\").strip()\n",
    "                    comment = str(change.get(\"comment\", \"\") or \"\").strip()\n",
    "                    delta = _size_delta(change)\n",
    "\n",
    "                    edits.append({\n",
    "                        \"user\": change.get(\"user\", \"\"),\n",
    "                        \"title\": title,\n",
    "                        \"comment\": comment,\n",
    "                        \"timestamp\": change.get(\"timestamp\", 0),\n",
    "                        \"delta\": int(delta),\n",
    "                    })\n",
    "\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ stream error: {e}\")\n",
    "\n",
    "    return pd.DataFrame(edits, columns=[\"user\",\"title\",\"comment\",\"timestamp\",\"delta\"])\n",
    "\n",
    "def _size_delta(change: dict) -> int:\n",
    "    \"\"\"Return absolute byte change for the edit if available, else 0.\"\"\"\n",
    "    length = change.get(\"length\") or {}\n",
    "    old = length.get(\"old\"); new = length.get(\"new\")\n",
    "    if isinstance(old, int) and isinstance(new, int):\n",
    "        return abs(new - old)\n",
    "    rev = change.get(\"revision\") or {}\n",
    "    osz = (rev.get(\"old\") or {}).get(\"size\")\n",
    "    nsz = (rev.get(\"new\") or {}).get(\"size\")\n",
    "    if isinstance(osz, int) and isinstance(nsz, int):\n",
    "        return abs(nsz - osz)\n",
    "    return 0\n",
    "\n",
    "time_edits = pd.DataFrame(columns=[\"Minute\",\"Number of edits\",\"Anomaly\"])\n",
    "minute = BATCH_SECONDS/60\n",
    "#continuous loop\n",
    "\n",
    "\n",
    "while True:\n",
    "    batch_df = collect_for(BATCH_SECONDS)\n",
    "    print(f\"✅ Collected {len(batch_df)} edits in the last {BATCH_SECONDS} seconds.\") \n",
    "    new_row = pd.DataFrame({'Number of edits': [len(batch_df)], 'Minute': [minute], 'Anomaly': [0]})\n",
    "    time_edits = pd.concat([time_edits,new_row])\n",
    "    minute = minute+0.25\n",
    "\n",
    "    time_edits.to_csv(\"time_edits.csv\", index=False)\n",
    "    if len(time_edits) > 1: \n",
    "        X = time_edits[['Minute','Number of edits']]\n",
    "        model = IsolationForest(contamination=0.05, random_state=42)\n",
    "        model.fit(X)\n",
    "        scores = model.decision_function(X)\n",
    "        outliers = np.argwhere(scores < np.percentile(scores, 5)).flatten()\n",
    "        time_edits['Anomaly'] = 0\n",
    "        time_edits.iloc[outliers, time_edits.columns.get_loc('Anomaly')] = 1\n",
    "\n",
    "\n",
    "#time analysis using sklearn.\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "time_edits = pd.read_csv(\"time_edits.csv\")\n",
    "X = time_edits[['Minute','Number of edits']]\n",
    "# Define the model and set the contamination level\n",
    "model = IsolationForest(contamination=0.05)\n",
    "model.fit(X)\n",
    "scores = model.decision_function(X)\n",
    "# Identify the points with the highest outlier scores\n",
    "outliers = np.argwhere(scores < np.percentile(scores, 5)).flatten()\n",
    "# Plot anomly\n",
    "colors=['green','red']\n",
    "\n",
    "for i in range(len(X)):\n",
    "    if i not in outliers:\n",
    "        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[0]) # Not anomly\n",
    "    else:\n",
    "        plt.scatter(X.iloc[i,0], X.iloc[i,1], color=colors[1]) # anomly\n",
    "plt.xlabel('Minutes',fontsize=13)\n",
    "plt.ylabel('Number of Edits',fontsize=13)        \n",
    "plt.title('Anomaly by Isolation Forest',fontsize=16)        \n",
    "plt.show()\n",
    "\n",
    "time_edits['Anomaly'] = 0\n",
    "time_edits.loc[outliers, 'Anomaly'] = 1\n",
    "time_edits.to_csv(\"time_edits_with_anomaly.csv\", index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bdb026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b1534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
